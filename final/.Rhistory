d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
HitoTDF <- abc}
else{
HitoTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
library(jiebaR)
message <- list()
HitoTDF <- data.frame()
SBTDF <- data.frame()
for(i in c(1:length(Hito))){
html <- htmlParse(GET(Hito.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
HitoTDF <- abc}
else{
HitoTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
library(jiebaR)
message <- list()
HitoTDF <- data.frame()
SBTDF <- data.frame()
for(i in c(1:length(Hito))){
html <- htmlParse(GET(Hito.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
HitoTDF <- abc}
else{
HitoTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
for(i in c(1:length(SB))){
html <- htmlParse(GET(SB.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
SBTDF <- abc}
else{
SBTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
install.packages("jiebaR")
install.packages("jiebaR")
library(jiebaR)
library(jiebaRD)
library(jiebaR)
message <- list()
HitoTDF <- data.frame()
SBTDF <- data.frame()
for(i in c(1:length(Hito))){
html <- htmlParse(GET(Hito.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
HitoTDF <- abc}
else{
HitoTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(tm)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(dplyr)
from <- 485
to   <- 501
prefix = "https://www.ptt.cc/bbs/TW_Entertain/index"
data <- list()
title <- list()
for (id in c(from:to))
{
url <- paste0(prefix, as.character(id), ".html" )
html<- htmlParse(GET(url))
url.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlAttrs)
title.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlValue)
data <- rbind(data, as.matrix(paste("www.ptt.cc", url.list, sep='')))
title <- rbind(title, as.matrix(title.list))
}
data <- unlist(data)
title <- unlist(title)
head(data)
head(title)
Hito <- c()
Hito.url <- c()
HitoT <- grep("綜藝大熱門", title)
Hito <- c(Hito,title[HitoT])
Hito.url <- c(Hito.url, data[HitoT])
SB <- c()
SB.url <- c()
SBT <- grep("小明星大跟班", title)
SB <- c(SB,title[SBT])
SB.url <- c(SB.url, data[SBT])
message <- list()
HitoTDF <- data.frame()
SBTDF <- data.frame()
for(i in c(1:length(Hito))){
html <- htmlParse(GET(Hito.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
d.corpus <- VCorpus( VectorSource(message) )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
abc <- data.frame(table(cc[as.character(d.corpus)]))
colnames(abc) <- c("word", as.character(i))
if(i == 1){
HitoTDF <- abc}
else{
HitoTDF <- merge(HitoTDF, abc, by = "word", all = T)}
}
pr <- predict(model, test,  OOB=TRUE, type = "response")
install.packages("keras")
library(keras)
install_tensorflow()
# Read in MNIST data
mnist <- dataset_mnist()
# Read in CIFAR10 data
cifar10 <- dataset_cifar10()
# Read in IMDB data
imdb <- dataset_imdb()
sapply(iris, class)
class(iris)
str(iris)
levels(iris$Species)
summary(iris$Species)
if (require(dplyr)){
distinct(iris, Species)
}
levels(iris$Species)
CO2_2000 %>%
filter(CO2..kt. == max(.$CO2..kt.)) %>%
.$Country.Code
filter(CO2_2000, CO2..kt. == max(CO2_2000$CO2..kt.))$Country.Code
library(RCurl)
library(bitops)
library(twitteR)
library(tm)
library(NLP)
library(wordcloud)
library(RColorBrewer)
library(textclean)
library(ggplot2)
consumerKey <- "eIaUSZ0CW1baBIawv6aO3yGhA"
consumerSecret <- "U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a"
accessToken <- "978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig"
accessSecret <- "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
nba_tweets <- searchTwitter("NBA", n = 1000, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
nba_tweets_text <- replace_emoji(nba_tweets_text)
for(i in seq(nba_tweets_text)){
nba_tweets_text[[i]]<-gsub('[[:punct:]]', '', nba_tweets_text[[i]])
nba_tweets_text[[i]]<-gsub("，"," ", nba_tweets_text[[i]])
nba_tweets_text[[i]]<-gsub("-"," ", nba_tweets_text[[i]])
nba_tweets_text[[i]]<-gsub("U+1F525"," ", nba_tweets_text[[i]])
}
View(nba_tweets)
str(nba_tweets_text)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
View(nba_corpus)
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, stopwords("english"))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeNumbers)
nba_corpus_clean <- tm_map(nba_corpus_clean, stripWhitespace)
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(8, 0.5), colors = rainbow(50))
View(nba_corpus_clean)
dfnba <- data.frame(nba_corpus_clean)
dfnba <- data.frame(nba_tweets_text)
View(dfnba)
dfnba_clean <- tm_map(nba_tweets_text, removePunctuation)
dfnba_clean <- tm_map(dfnba_clean, content_transformer(tolower))
dfnba_clean <- tm_map(dfnba_clean, removeWords, stopwords("english"))
dfnba_clean <- tm_map(dfnba_clean, removeNumbers)
dfnba_clean <- tm_map(dfnba_cleann, stripWhitespace)
dfnba <- data.frame(nba_tweets_text)
View(dfnba)
dfnba <- data.frame(nba_corpus_clean)
dfnba <- data.frame(text=unlist(sapply(nba_corpus_clean, '[', "content")), stringsAsFactors = F)
View(dfnba)
dfnba <- data.frame(text=unlist(sapply(nba_corpus_clean, as.character)), stringsAsFactors = F)
View(dfnba)
lebron <- grep("lebron", nba_corpus_clean)
lebron <- grep("lebron", nba_tweets_text)
lebron
curry <- grep("curry", nba_tweets_text)
curry
nba_tweets_text[748]
nba_tweets_text[71]
wordcloud2(data = nba_corpus_clean,backgroundColor = 'black', color = 'random-light',
minRotation = -pi/2, maxRotation = pi/2,
rotateRatio = 1)
install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(data = nba_corpus_clean,backgroundColor = 'black', color = 'random-light',
minRotation = -pi/2, maxRotation = pi/2,
rotateRatio = 1)
wordcloud2(data = nba_tweets_text,backgroundColor = 'black', color = 'random-light',
minRotation = -pi/2, maxRotation = pi/2,
rotateRatio = 1)
data <- list()
title <- list()
date <- list()
#抓取三月ptt NBA版的貼文
for( i in c(5702:5794)){
tmp <- paste(i, '.html', sep='')
url <- paste('www.ptt.cc/bbs/NBA/index', tmp, sep='')
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlValue)
url.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlAttrs)
date.list <- xpathSApply(html, "//div[@class='meta']/div[@class='date']", xmlValue)
data <- rbind(data, as.matrix(paste('www.ptt.cc', url.list, sep='')))
title <- rbind(title, as.matrix(title.list))
date <- rbind(date, as.matrix(date.list))
}
# data 存網址  title 存標題 #date 存時間
data <- unlist(data)
title <- unlist(title)
date <- unlist(date)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(tm)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(dplyr)
data <- list()
title <- list()
date <- list()
#抓取三月ptt NBA版的貼文
for( i in c(5702:5794)){
tmp <- paste(i, '.html', sep='')
url <- paste('www.ptt.cc/bbs/NBA/index', tmp, sep='')
html <- htmlParse(GET(url),encoding = "UTF-8")
title.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlValue)
url.list <- xpathSApply(html, "//div[@class='title']/a[@href]", xmlAttrs)
date.list <- xpathSApply(html, "//div[@class='meta']/div[@class='date']", xmlValue)
data <- rbind(data, as.matrix(paste('www.ptt.cc', url.list, sep='')))
title <- rbind(title, as.matrix(title.list))
date <- rbind(date, as.matrix(date.list))
}
# data 存網址  title 存標題 #date 存時間
data <- unlist(data)
title <- unlist(title)
date <- unlist(date)
Lebron <- c()
Lebron.url <- c()
Lebron.date <- c()
# 找出有關鍵字的標題並分類
lebron1 <- grep("James", title)
lebron2 <- grep("LBJ", title)
lebron3 <- grep("姆斯", title)
Lebron <- c(Lebron,title[lebron1])
Lebron <- c(Lebron,title[lebron2])
Lebron <- c(Lebron,title[lebron3])
Lebron.url <- c(Lebron.url, data[lebron1])
Lebron.url <- c(Lebron.url, data[lebron2])
Lebron.url <- c(Lebron.url, data[lebron3])
Lebron.date <- c(Lebron.date, date[lebron1])
Lebron.date <- c(Lebron.date, date[lebron2])
Lebron.date <- c(Lebron.date, date[lebron3])
message <- list()
cc = worker()
LBJTDF <- data.frame()
SCTDF <- data.frame()
JHTDF <- data.frame()
player <- c()
postdate <- c()
hot <- c()
for(i in c(1:length(Lebron))){
html <- htmlParse(GET(Lebron.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
message
for(i in c(1:length(Lebron))){
html <- htmlParse(GET(Lebron.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
for(i in c(1:length(Lebron))){
html <- htmlParse(GET(Lebron.url[i]),encoding = "UTF-8")
message.list <- xpathSApply(html, "//div[@class='push']/span[@class='f3 push-content']", xmlValue)
message <- unlist(message.list)
#日期分類
player <- c(player,"LBJ")
if(grepl("3/0",Lebron.date[i])== TRUE){
postdate <- c(postdate, "3/01~3/09") }
else if(grepl("3/1",Lebron.date[i])== TRUE){
postdate <- c(postdate, "3/10~3/19")}
else if(grepl("3/2",Lebron.date[i])== TRUE){
postdate <- c(postdate, "3/20~3/29")}
else if(grepl("3/3",Lebron.date[i])== TRUE){
postdate <- c(postdate, "3/30~3/31")}
if(length(message) > 100){
hot <- c(hot, "Boom!")}
else if(length(message) > 75){
hot <- c(hot, "Hot")}
else if(length(message) > 50){
hot <- c(hot, "Soso")}
else{
hot <- c(hot, "Cold") }
message
library(randomForest)
library(corrplot)
library(party)
setwd("C:/Users/User/Desktop/R/R/final")
data <- read.csv("all0618.csv")
test <- read.csv("all_test.csv")
data$分類 <- factor(data$分類)
data$來賓 <- factor(data$來賓)
set.seed(102)
model <- cforest(factor(g_view) ~ 分類 + 來賓, data = data)
View(data)
qplot(x=日期,
y=c_view,
data=data,
geom="point",                         # 圖形=scatter plot
main = "觀看次數",
xlab="date",
ylab="view",
# 以顏色標註月份，複合式的散布圖
)
library(ggplot2)
qplot(x=日期,
y=c_view,
data=data,
geom="point",                         # 圖形=scatter plot
main = "觀看次數",
xlab="date",
ylab="view",
# 以顏色標註月份，複合式的散布圖
)
qplot(x=日期,
y=c_view,
data=data,
geom="point",                         # 圖形=scatter plot
main = "觀看次數",
xlab="date",
ylab="view",
color = color                        # 以顏色標註月份，複合式的散布圖
)
qplot(x=日期,
y=c_view,
data=data,
geom="point",                         # 圖形=scatter plot
main = "觀看次數",
xlab="date",
ylab="view",
color = 分類                        # 以顏色標註月份，複合式的散布圖
)
data <- read.csv("all0618.csv")
qplot(x=日期,
y=c_view,
data=data,
geom="point",                         # 圖形=scatter plot
main = "觀看次數",
xlab="date",
ylab="view",
color = 分類                        # 以顏色標註月份，複合式的散布圖
)
ggsave("plot.png", width = 4, height = 3)
ggsave("plot.png", width = 10, height = 3)
ggsave("plot.png", width = 20, height = 10)
canvas <- ggplot(data=data)
# 準備畫布
ggplot(data=airquality) +
# 散布圖對應的函式是geom_point()
geom_point(aes(x=日期,  # 用aes()，描繪散布圖內的各種屬性
y=c_view,
main="Scatter Plot of Ozone-Temp",
color = 分類)
) +
# 用geom_smooth()加上趨勢線
geom_smooth(aes(x=日期,
y=c_view)) +
# 用labs()，進行文字上的標註(Annotation)
labs(title="Scatter of Temp-Ozone",
x="Temp",
y="Ozone")
ggplot(data=data) +
# 散布圖對應的函式是geom_point()
geom_point(aes(x=日期,  # 用aes()，描繪散布圖內的各種屬性
y=c_view,
main="Scatter Plot of Ozone-Temp",
color = 分類)
) +
# 用geom_smooth()加上趨勢線
geom_smooth(aes(x=日期,
y=c_view)) +
# 用labs()，進行文字上的標註(Annotation)
labs(title="Scatter of Temp-Ozone",
x="Temp",
y="Ozone")
sapply(data, 分類, sum())
sapply(data, data$分類, sum())
wife <- data(data$分類 == 夫妻)
wife
wife <- data(data$分類 == "夫妻")
data$分類
wife
wife <- data[data$分類 == "夫妻"]
wife
wife <- data[data$分類 == "夫妻",]
View(raw.titles)
View(wife)
活動 <- data[data$分類 == "活動",]
View(活動)
wife <- data[data$分類 == "夫妻",]
活動 <- data[data$分類 == "活動",]
遊戲 <- data[data$分類 == "遊戲",]
歌曲 <- data[data$分類 == "歌曲",]
模仿 <- data[data$分類 == "模仿",]
談話性 <- data[data$分類 == "談話性",]
職業 <- data[data$分類 == "職業",]
View(談話性)
夫妻平均 <- mean(wife$c_view)
夫妻平均
夫妻 <- data[data$分類 == "夫妻",]
夫妻平均 <- mean(wife$c_view)
活動 <- data[data$分類 == "活動",]
活動平均 <- mean(活動$c_view)
遊戲 <- data[data$分類 == "遊戲",]
遊戲平均 <- mean(遊戲$c_view)
歌曲 <- data[data$分類 == "歌曲",]
歌曲平均 <- mean(歌曲$c_view)
模仿 <- data[data$分類 == "模仿",]
模仿平均 <- mean(模仿$c_view)
談話性 <- data[data$分類 == "談話性",]
談話性平均 <- mean(談話性$c_view)
職業 <- data[data$分類 == "職業",]
職業平均 <- mean(職業$c_view)
夫妻平均
topic <- c("夫妻","活動","遊戲","歌曲","模仿","談話性","職業")
view <- c(夫妻平均,活動平均,遊戲平均,歌曲平均,模仿平均,談話性平均,職業平均)
topic
view
group <- data.frame(topic,view)
View(group)
qplot(view, data = gruop, geom = "histogram",
binwidth = 0.5, xlim = c(0, 3))
qplot(view, data = group, geom = "histogram",
binwidth = 0.5, xlim = c(0, 3))
qplot(view, data = group, geom = "histogram")
qplot(view, data = group, geom = "histogram",bins = 30)
qplot(view, data = group, geom = "histogram",binwidth = 30)
qplot(view, data = group, geom = "histogram",binwidth = 3)
qplot(view, data = group, geom = "bar")
qplot(topic, data = group, geom = "bar")
View(data)
qplot(分類, data = data, geom = "bar")
ggsave("分類.png", width = 20, height = 10)
ggsave("分類.png", width = 8, height = 6)
qplot(分類, data = group, geom = "bar", weight = view) +
ylab("平均觀看次數")
qplot(topic, data = group, geom = "bar", weight = view) +
ylab("平均觀看次數")
qplot(topic, data = group, geom = "bar", colour = topic,weight = view) +
ylab("平均觀看次數") + geom_text(stat="view",aes(label=..view..),vjust=-1)
qplot(topic, data = group, geom = "bar", colour = topic,weight = view) +
ylab("平均觀看次數")
qplot(topic, data = group, geom = "bar", fill = topic,weight = view) +
ylab("平均觀看次數")
ggsave("平均觀看次數.png", width = 8, height = 6)
head(table(guest))
guest
data3 <- read.csv("all0620.csv")
View(data3)
