point.df <- data.frame(
lat = 22.97 + rnorm(100)/800,
long = 120.23 + rnorm(100)/800,
size = runif(100, 5, 20),
color = sample(colors(), 100)
)
m <- leaflet(point.df) %>%
addTiles() %>%
setView(lng = 120.23, lat = 22.97, zoom = 17)
m %>% addCircleMarkers(radius = ~size, color = ~color, fill = FALSE)
m <- leaflet() %>% setView(lng=120.239, lat=22.992, zoom = 12)
m %>% addTiles()
m <- leaflet() %>% setView(lng=120.239, lat=22.992, zoom = 12)
m %>% addTiles()
m <- leaflet() %>% setView(lng=121.560, lat=25.027, zoom = 12)
m %>% addTiles()
m <- leaflet() %>% setView(lng=121.560, lat=25.027, zoom = 20)
m %>% addTiles()
install.packages("igraph")
library(igraph)
g1 <- graph( c(0,1, 1,2, 2,3, 3,4))
plot(g1,layout=layout.circle(g1))
g1 <- graph( c(0,1, 1,2, 2,3, 3,4))
g1 <- make_graph( c(0,1, 1,2, 2,3, 3,4))
g1 <- make_graph(c(1, 2, 2, 3, 3, 4, 5, 6))
plot(g1,layout=layout.circle(g1))
g1 <- make_graph(c(1, 2, 2, 3, 3, 4,4,5,6))
plot(g1,layout=layout.circle(g1))
g1 <- make_graph(c(1, 2, 2, 3, 3, 4,4,5))
g1 <- make_graph(c(1, 2, 2, 3, 3, 4,4,5))
plot(g1,layout=layout.circle(g1))
g2 <- graph.star(10, mode = "in")
plot(g2,layout=layout.fruchterman.reingold(g2))
traits <- read.csv('http://igraph.sourceforge.net/igraphbook/traits.csv', head=FALSE)
names(traits) <- c('name','age','gender')
traits[,1] <- sapply(strsplit(as.character(traits[,1]),' '),'[',1)
relation <- read.csv('http://igraph.sourceforge.net/igraphbook/relations.csv', head=FALSE)
names(relation) <- c('from','to','sameroom','friendship','advice')
g4 <- graph.data.frame(relation,vertices=traits)
plot(g4,layout=layout.kamada.kawai,vertex.shape='rectangle',vertex.label=V(g4)$name,vertex.size=20,asp=F)
install.packages("twitteR")
install.packages("ROAuth")
library(twitteR)
library(ROAuth)
setup_twitter_oauth( "eIaUSZ0CW1baBIawv6aO3yGhA", 	"U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a", "	978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig
", "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4")
setup_twitter_oauth( "eIaUSZ0CW1baBIawv6aO3yGhA", 	"U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a", "	978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig
", "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4")
setup_twitter_oauth( "eIaUSZ0CW1baBIawv6aO3yGhA", 	"U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a",
"978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig", "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4")
tweets <- userTimeline("RDataMining", n = 3200)
tweets
tweets <- userTimeline("NBA", n = 3200)
tweets <- userTimeline("Lebron", n = 3200)
tweets <- userTimeline("RDataMining", n = 3200)
tweet <- userTimeline("trust the process", n = 3200)
tweet <- userTimeline("nba", n = 200)
tweet
TheProcess <- userTimeline("TheProcess", n = 200)
TheProcess
tweet <- userTimeline("nba", n = 3200)
url <- "http://www.rdatamining.com/data/RDataMining-Tweets-20160212.rds"
download.file(url, destfile = "./data/RDataMining-Tweets-20160212.rds")
download.file(url, destfile = "C:\Users\User\Desktop\R\R\week_4\Twitter\RDataMining-Tweets-20160212.rds")
download.file(url, destfile = "Users\User\Desktop\R\R\week_4\Twitter\RDataMining-Tweets-20160212.rds")
download.file(url, destfile = "Desktop\R\R\week_4\Twitter\RDataMining-Tweets-20160212.rds")
download.file(url)
example(download.file)
setwd("C:/Users/User/Desktop/R/R/week_4/Twitter")
download.file(url, destfile = "RDataMining-Tweets-20160212.rds")
tweets <- readRDS("./data/RDataMining-Tweets-20160212.rds")
tweets <- readRDS("RDataMining-Tweets-20160212.rds")
tweets <- readRDS("RDataMining-Tweets-20160212.rds")
tweets <- readRDS("RDataMining-Tweets-20160212.rds")
(n.tweet <- length(tweets))
tweets.df <- twListToDF(tweets)
tweets.df
tweets.df[190, c("id", "created", "screenName", "replyToSN",
"favoriteCount", "retweetCount", "longitude", "latitude", "text")]
writeLines(strwrap(tweets.df$text[190], 60))
install.packages("tm")
library(tm)
install.packages("NLP")
install.packages("NLP")
library(tm)
library(NLP)
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[190]]$content, 60))
stemCompletion2 <- function(x, dictionary) f
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
g
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[190]]$content, 60))
library(devtools)
library(twitteR)
library(data.table)
install.packages("data.table")
library(data.table)
library(devtools)
library(twitteR)
library(data.table)
consumerKey <- "eIaUSZ0CW1baBIawv6aO3yGhA"
consumerSecret <- "U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a"
accessToken <- "978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig"
accessSecret <- "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4"
options(httr_oauth_cache=T) # This will enable the use of a local file to cache OAuth access credentials between R sessions.
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
tweets <- searchTwitter('#airbnb',
n=50,
since = '2018-03-13',
until = '2018-03-18')
tweets
tweets.list <- twListToDF(tweets)
names.list <-  rbindlist(lapply(tweets.list$screenName,
as.data.frame))
names(names.list)[1] <- "Name"
alldata <- data.frame()
for (i in 1:3){ #Cursor
tryCatch(
{
# get name from '#_______' users list
tag.user <- names.list$Name[i]
# print query location
print(paste(i, tag.user))
# get User's twitter account
tag.user.account <- getUser(tag.user)
# get account's friend (if accessible)
user.friends <- tag.user.account$getFriends(retryOnRateLimit=180)
print(length(user.friends))
# limit
if (length(user.friends) < 3000){
# Make data.table of user's friends data list.
friends.df <- rbindlist(lapply(user.friends, as.data.frame))
# Get the only friends name column.
friends.name.df <- data.frame(tempname=c(friends.df$name))
# Change column name
colname <- toString(tag.user)
setnames(friends.name.df, c(colname))
# Write table
write.table(friends.name.df, file = paste(colname, ".csv"))
# bind data in the same data.frame
alldata <- rbind.fill(alldata, friends.name.df)
#data <- cbind(list(data, friends.name.df))
}
else{
print(paste(i, tag.user, "<== friends count > 500"))
}
},
warning = function(w){},
error = function(e){
#ERROR (need to store it?)
print(paste("ERROR", tag.user))
},
finally = {
print("End Try&Catch")
})
i = i+1
}
tweets <- searchTwitter('#nba',
n=50,
since = '2018-03-19',
until = '2018-03-26')
tweets
tweets.list <- twListToDF(tweets)
names.list <-  rbindlist(lapply(tweets.list$screenName,
as.data.frame))
tweets.list$screenName
names.list
tweets.list
names(names.list)[1] <- "Name"
names.list
tweets.list
alldata <- data.frame()
for (i in 1:3){ #Cursor
tryCatch(
{
# get name from '#_______' users list
tag.user <- names.list$Name[i]
# print query location
print(paste(i, tag.user))
# get User's twitter account
tag.user.account <- getUser(tag.user)
# get account's friend (if accessible)
user.friends <- tag.user.account$getFriends(retryOnRateLimit=180)
print(length(user.friends))
# limit
if (length(user.friends) < 3000){
# Make data.table of user's friends data list.
friends.df <- rbindlist(lapply(user.friends, as.data.frame))
# Get the only friends name column.
friends.name.df <- data.frame(tempname=c(friends.df$name))
# Change column name
colname <- toString(tag.user)
setnames(friends.name.df, c(colname))
# Write table
write.table(friends.name.df, file = paste(colname, ".csv"))
# bind data in the same data.frame
alldata <- rbind.fill(alldata, friends.name.df)
#data <- cbind(list(data, friends.name.df))
}
else{
print(paste(i, tag.user, "<== friends count > 500"))
}
},
warning = function(w){},
error = function(e){
#ERROR (need to store it?)
print(paste("ERROR", tag.user))
},
finally = {
print("End Try&Catch")
})
i = i+1
}
install.packages("RCurl")
library(RCurl)
library(twitteR)
install.packages("bitops")
install.packages("bitops")
library(RCurl)
library(twitteR)
library(bitops)
library(RCurl)
consumerKey <- "eIaUSZ0CW1baBIawv6aO3yGhA"
consumerSecret <- "U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a"
accessToken <- "978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig"
accessSecret <- "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
nba_tweets <- search("NBA", n = 100, lang= "en")
nba_tweets <- searchTwitter("NBA", n = 100, lang= "en")
nba_tweets
nba_tweets [1:3]
library(tm)
library(NLP)
library(tm)
install.packages("wordcloud")
library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
library(wordcloud)
class(nba_tweets)
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
str(nba_tweets_text)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
library(tm)
library(tm)
library(NLP)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
nba_corpus
inspect(nba_corpus[1])
inspect(nba_corpus[66])
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
inspect(nba_corpus_clean[66])
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
inspect(nba_corpus_clean[Harden])
inspect(nba_corpus_clean["Harden"])
inspect(nba_corpus_clean[8])
inspect(nba_corpus_clean[2])
library(wordcloud)
nba_corpus_clean
nba_tweets <- searchTwitter("NBA", n = 10, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
str(nba_tweets_text)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
inspect(nba_corpus[1])
inspect(nba_corpus[66])
library(twitteR)
nba_tweets <- searchTwitter("NBA", n = 10, lang= "en")
consumerKey <- "eIaUSZ0CW1baBIawv6aO3yGhA"
consumerSecret <- "U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a"
accessToken <- "978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig"
accessSecret <- "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
nba_tweets <- searchTwitter("NBA", n = 10, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
str(nba_tweets_text)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
inspect(nba_corpus[1])
inspect(nba_corpus[66])
inspect(nba_corpus[5])
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
inspect(nba_corpus_clean[8])
inspect(nba_corpus_clean[3])
nba_corpus_clean[-3]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <-nba_corpus_clean[-3]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, stopwords("english"))
inspect(nba_corpus_clean[5])
nba_corpus_clean <- tm_map(nba_corpus_clean, removeNumbers)
nba_corpus_clean <- tm_map(nba_corpus_clean, stripWhitespace)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean)
use warnings()
use warnings(nba_corpus_clean)
warnings()
nba_tweets <- searchTwitter("NBA", n = 100, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
str(nba_tweets_text)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
inspect(nba_corpus[1])
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
library(RCurl)
library(bitops)
library(twitteR)
library(tm)
library(NLP)
library(wordcloud)
library(RColorBrewer)
library(bitops)
library(RCurl)
library(twitteR)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
consumerKey <- "eIaUSZ0CW1baBIawv6aO3yGhA"
consumerSecret <- "U8ceWsFc8lymqRtfhK1b1QQR1DFvDUPUORbBNkYy4ziOiOhk3a"
accessToken <- "978170993544806400-lb7cs4vPkCtbHqwCO6vynaMceKRs0Ig"
accessSecret <- "ewPYmyyGWce0ZgOpITkqJxJ8hVbKzkSZqecSG6Tjx6wD4"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
nba_tweets <- searchTwitter("NBA", n = 100, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
Encoding(nba_corpus_clean) <- "UTF-8"
str(nba_tweets_text)
Encoding(nba_tweets_text) <- "UTF-8"
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
str_conv(nba_tweets_text, "UTF-8")
install.packages("stringr")
library(stringr)
str_conv(nba_tweets_text, "UTF-8")
warnings()
nba_tweets_text[-1, -2 ,-3, -4 ,-5, -6 ,-7, -8, -9 ,10, -11,-12, -13, -14, -15,-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26]
nba_tweets_text[-1]
nba_tweets_text <-nba_tweets_text[-1]
nba_tweets_text <-nba_tweets_text[-2]
nba_tweets_text <-nba_tweets_text[-3]
nba_tweets_text <-nba_tweets_text[-4]
nba_tweets_text <-nba_tweets_text[-5]
nba_tweets_text <-nba_tweets_text[-6]
nba_tweets_text <-nba_tweets_text[-7]
nba_tweets_text <-nba_tweets_text[-8]
nba_tweets_text <-nba_tweets_text[-9]
nba_tweets_text <-nba_tweets_text[-10]
nba_tweets_text <-nba_tweets_text[-11]
nba_tweets_text <-nba_tweets_text[-12]
nba_tweets_text <-nba_tweets_text[-13]
nba_tweets_text <-nba_tweets_text[-14]
nba_tweets_text <-nba_tweets_text[-15]
nba_tweets_text <-nba_tweets_text[-16]
nba_tweets_text <-nba_tweets_text[-17]
nba_tweets_text <-nba_tweets_text[-18]
nba_tweets_text <-nba_tweets_text[-19]
nba_tweets_text <-nba_tweets_text[-20]
nba_tweets_text <-nba_tweets_text[-21]
nba_tweets_text <-nba_tweets_text[-22]
nba_tweets_text <-nba_tweets_text[-23]
nba_tweets_text <-nba_tweets_text[-24]
nba_tweets_text <-nba_tweets_text[-25]
nba_tweets_text <-nba_tweets_text[-26]
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
inspect(nba_corpus[1])
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
str_conv(nba_tweets_text, "UTF-8")
warnings()
nba_corpus_clean[-19]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- nba_corpus_clean[-19]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- nba_corpus_clean
nba_corpus_clean <- nba_corpus_clean[-19]
nba_corpus_clean <- nba_corpus_clean[-23]
nba_corpus_clean <- nba_corpus_clean[-24]
nba_corpus_clean <- nba_corpus_clean[-30]
nba_corpus_clean <- nba_corpus_clean[-45]
nba_corpus_clean <- nba_corpus_clean[-48]
nba_corpus_clean <- nba_corpus_clean[-58]
nba_corpus_clean <- nba_corpus_clean[-73]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean
nba_corpus_clean <- nba_corpus_clean[-19]
nba_corpus_clean <- nba_corpus_clean[-23]
nba_corpus_clean <- nba_corpus_clean[-24]
nba_corpus_clean <- nba_corpus_clean[-30]
nba_corpus_clean <- nba_corpus_clean[-45]
nba_corpus_clean <- nba_corpus_clean[-48]
nba_corpus_clean <- nba_corpus_clean[-58]
nba_corpus_clean <- nba_corpus_clean[-73]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- nba_corpus_clean[-23]
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- nba_corpus_clean[-23]
nba_corpus_clean[23]
View(nba_corpus_clean)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, stopwords("english"))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeNumbers)
nba_corpus_clean <- tm_map(nba_corpus_clean, stripWhitespace)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean)
wordcloud(nba_corpus_clean)
Taiwan_tweets <- searchTwitter("Taiwan", n = 100, lang= "en")
Taiwan_tweets_text <- sapply(Taiwan_tweets, function(x) x$getText())
str(Taiwan_tweets_text)
str_conv(nba_tweets_text, "UTF-8")
Taiwan_corpus <- Corpus(VectorSource(Taiwan_tweets_text))
inspect(nba_corpus[1])
inspect(Taiwan_corpus [1])
Taiwan_corpus_clean <- tm_map(Taiwan_corpus, removePunctuation)
Taiwan_corpus_clean <- tm_map(Taiwan_corpus_clean, content_transformer(tolower))
Taiwan_corpus_clean <- tm_map(Taiwan_corpus_clean, removeWords, stopwords("english"))
Taiwan_corpus_clean <- tm_map(Taiwan_corpus_clean, removeNumbers)
Taiwan_corpus_clean <- tm_map(Taiwan_corpus_clean, stripWhitespace)
wordcloud(nba_corpus_clean)
wordcloud(Taiwan_corpus_clean)
Taiwan_corpus_clean <- tm_map(Taiwan_corpus_clean, removeWords, c("taiwan"))
wordcloud(Taiwan_corpus_clean)
wordcloud(Taiwan_corpus_clean)
wordcloud(Taiwan_corpus_clean)
wordcloud(Taiwan_corpus_clean)
library(RCurl)
nba_tweets <- searchTwitter("NBA", n = 1000, lang= "en")
nba_tweets_text <- sapply(nba_tweets, function(x) x$getText())
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, stopwords("english"))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeNumbers)
nba_corpus_clean <- tm_map(nba_corpus_clean, stripWhitespace)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean)
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(3, 0.5), colors = rainbow(50))
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(4, 0.5), colors = rainbow(50))
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(8, 0.5), colors = rainbow(50))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(8, 0.5), colors = rainbow(50))
install.packages("gsub")
install.packages("grep")
nba_tweets_text <- gsub("\\\w+ *", " ", nba_tweets_text)
nba_tweets_text <- gsub("[\u{1F300}-\u{1F5FF}|\u{1F1E6}-\u{1F1FF}|\u{2700}-\u{27BF}|\u{1F900}-\u{1F9FF}|\u{1F600}-\u{1F64F}|\u{1F680}-\u{1F6FF}|\u{2600}-\u{26FF}] ", " ", nba_tweets_text)
gsub('\\p{So}|\\p{Cn}', '', nba_tweets, perl = TRUE)
gsub('\\p{So}|\\p{Cn}', '', nba_tweets_text, perl = TRUE)
nba_corpus <- Corpus(VectorSource(nba_tweets_text))
nba_corpus_clean <- tm_map(nba_corpus, removePunctuation)
nba_corpus_clean <- tm_map(nba_corpus_clean, content_transformer(tolower))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, stopwords("english"))
nba_corpus_clean <- tm_map(nba_corpus_clean, removeNumbers)
nba_corpus_clean <- tm_map(nba_corpus_clean, stripWhitespace)
nba_corpus_clean <- tm_map(nba_corpus_clean, removeWords, c("nba"))
wordcloud(nba_corpus_clean, random.order = F, max.words = 40, scale = c(8, 0.5), colors = rainbow(50))
grep("nba", nba_tweets_text)
grep("pts", nba_tweets_text)
gsub('\u', ' ', nba_tweets_text, perl = TRUE)
gsub('\\u', ' ', nba_tweets_text, perl = TRUE)
gsub('\\\u', ' ', nba_tweets_text, perl = TRUE)
gsub('\\u{1F300}', ' ', nba_tweets_text, perl = TRUE)
gsub('\\u{1F300}', ' ', nba_tweets_text)
grep("\u{1F300}", nba_tweets_text)
nba_corpus_clean <- tm_map(nba_corpus_clean, toSpace ,"\u")
nba_corpus_clean <- tm_map(nba_corpus_clean, toSpace ,"\n")
nba_corpus_clean <- tm_map(nba_corpus_clean, toSpace ,"nba")
library(tm)
library(NLP)
library(tm)
nba_corpus_clean <- tm_map(nba_corpus_clean, toSpace ,"nba")
library(httr)
library(rjson)
library(httpuv)
library(Rfacebook)
library(plyr)
library(NLP)
library(tm)
library(rvest)
library(xml2)
library(rvest)
library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
install.packages("rJava")
install.packages("SnowballC")
library(rJava)
library(SnowballC)
